Terminal AI Assistant Setup

Overview:
- Tool: llm (version 0.28 as of Dec 2025) with llm-ollama plugin for remote Ollama access.
- Remote Ollama server: Windows PC (192.168.0.7:11434) with GTX 4090 GPU acceleration.
- Local client: Beelink Ubuntu Server (headless).
- Purpose: Reliable terminal CLI assistant for homelab sysadmin tasks (code/scripts + brief explanations; no agent execution/loops).

Installation (on Beelink):
1. sudo apt install pipx -y
2. pipx ensurepath  # Reload shell or logout/login
3. pipx install llm
4. llm install llm-ollama
5. Add to ~/.bashrc: export OLLAMA_HOST=http://192.168.0.7:11434
   source ~/.bashrc

Custom Template (~/.config/llm/templates/homelab-sysadmin.yaml):
system: |
  You are an expert Linux sysadmin for a headless Ubuntu Server homelab on a Beelink mini-PC running a Docker-based media stack.
  Prioritize reliability, efficiency, open-source solutions, and working around hardware quirks.
  Always provide the clean, executable bash command or script first.
  Then add a brief explanation of key parts if it helps understanding—keep it concise, no fluff.
  Never summarize or truncate outputs; provide full raw content when needed.
  Do not hallucinate commands or configs. Use direct, precise language.

model: gemma3:27b  # Default; override with -m qwen3:32b or others as needed

Aliases (add to ~/.bashrc):
alias localai='llm -t ~/.config/llm/templates/homelab-sysadmin.yaml'
alias localai-chat='llm chat -t ~/.config/llm/templates/homelab-sysadmin.yaml'

Usage:
- Single query: localai "natural language request"
- Interactive multi-turn: localai-chat (great for script → cron follow-ups)
- Override model: localai -m qwen3:32b "request"

Notes:
- Full raw output streaming; no auto-execution (manual copy-paste for safety/review).
- GPU offload confirmed via VRAM usage on Windows Ollama.
- Short alias reuses old "localai" name for easy typing.