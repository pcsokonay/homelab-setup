Local AI Setup (OpenInterpreter + Ollama)

Overview:
- Purpose: Headless terminal AI assistant on Beelink for natural-language Ubuntu Server tasks (bash scripting, Docker management, UFW/iptables, log analysis, qBittorrent/SABnzbd tweaks).
- Architecture: Ollama server on chocksmainpc (GTX 4090 GPU acceleration). OpenInterpreter client on Beelink (remote API calls).

Ollama Server (chocksmainpc):
- Runs as service with OLLAMA_HOST=0.0.0.0 (network access enabled).
- API endpoint: http://100.108.206.83:11434/v1
- Primary model: gemma3:27b (fast reasoning/tool-use).

OpenInterpreter Client (Beelink - Ubuntu Server 24.04.3):
- Virtualenv: ~/oi_env (latest open-interpreter via pip)
- Profile: ~/.config/open-interpreter/profiles/beelink.yaml
- Launcher script: ~/localai_launcher.sh (executable)
- Alias: localai (in ~/.bashrc → points to the script)
- Profile YAML content (exact):
model: gemma3:27b
api_base: http://100.108.206.83:11434/v1
context_window: 32768
max_tokens: 4096
api_key: dummy
system_message: |
  [Full system prompt text from steps above – paste exactly]

Launcher script content:
#!/bin/bash
source ~/oi_env/bin/activate
interpreter --profile ~/.config/open-interpreter/profiles/beelink.yaml

Notes:
- pkg_resources warning: Harmless deprecation notice.
- For larger context: Edit YAML → model: llama3.1:70b and context_window: 128000 (test speed).
- LiteLLM API key warnings fixed via api_key: dummy.
- Profiles avoid CLI parsing issues with long prompts.

Last updated: January 2026