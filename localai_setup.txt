Local AI Setup (OpenInterpreter + Ollama)

Overview:
- Purpose: Headless terminal AI assistant on Beelink for natural-language Ubuntu Server tasks (bash scripting, Docker management, UFW, full log analysis without truncation, qBittorrent/SABnzbd tweaks).
- Architecture: Ollama server on chocksmainpc (GTX 4090). OpenInterpreter client on Beelink (remote API).

Ollama Server (chocksmainpc):
- Service with OLLAMA_HOST=0.0.0.0
- API: http://100.108.206.83:11434/v1
- Primary model: llama3.1:70b (128k context for full outputs) or gemma3:27b fallback

OpenInterpreter Client (Beelink):
- Virtualenv: ~/oi_env
- Launcher: ~/localai_launcher.sh (executable, CLI flags)
- Alias: localai (in ~/.bashrc)

Launcher script (exact):
#!/bin/bash
source ~/oi_env/bin/activate

SYSTEM_MESSAGE=$(cat << 'EOF'
[Refined prompt from above]
EOF
)

interpreter --model llama3.1:70b --api_base http://100.108.206.83:11434/v1 --context_window 128000 --max_tokens 8192 --api_key "" --system_message "$SYSTEM_MESSAGE"

Notes:
- No API key needed (empty suppresses messages).
- Large context eliminates truncation.
- Confirmation enabled (add -y for auto-run trusted commands).
- LiteLLM/Pydantic warnings harmless.
- Custom prompt enforces raw full outputs + conciseness.

Status: Fully working (January 2026)